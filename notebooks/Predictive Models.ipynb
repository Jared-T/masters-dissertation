{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "import os\n",
    "import tslearn\n",
    "\n",
    "file_path = os.path.join(\"..\", \"data\", \"2021 Full Merch Coords and Fuel Price with Flags.csv\")\n",
    "data = pd.read_csv(file_path, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f16dcf32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Transaction Date', 'REG_NUM', 'Merchant Name', 'Purchase Category',\n",
       "       'No. of Litres', 'Transaction Amount', 'Make', 'Model',\n",
       "       'Make and Model', 'Site', 'District', 'Category', 'Rental',\n",
       "       'Merchant Lat', 'Merchant Long', 'Site Lat', 'Site Long', 'Fuel Type',\n",
       "       'Actual Fuel Price', 'Actual Fuel Price Inland',\n",
       "       'Estimated Price Per Litre', 'Month Name', 'Weekday Name', 'YearMonth',\n",
       "       'AggClusterLabels', 'TransKmeansCluster', 'Average_Category_Amount',\n",
       "       'Transaction_Amount_Flag', 'Days_Between_Transactions',\n",
       "       'Transaction_Frequency_Flag', 'Fuel_Price_Flag', 'Flag', 'Reason'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a864fa27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reason\n",
       "PPL                         108517\n",
       "Clear                        55274\n",
       "PPL + Frequency              25199\n",
       "PPL + Amount                  4077\n",
       "Frequency                     2464\n",
       "PPL + Frequency + Amount      1338\n",
       "Amount                         255\n",
       "Frequency + Amount              52\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Reason'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f83c66",
   "metadata": {},
   "source": [
    "# Create the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e247f397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns from the data that we want to use\n",
    "data = data[['Transaction Amount', 'No. of Litres', 'District', 'Make and Model', 'Fuel Type', 'Flag', 'Category']]\n",
    "\n",
    "# Example of replacing spaces with underscores in the 'Make' column\n",
    "data['Make and Model'] = data['Make and Model'].str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b22887c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Flag\n",
       "True     141902\n",
       "False     55274\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Flag'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17505209",
   "metadata": {},
   "source": [
    "# Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "acea28f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 2. Data Preprocessing\n",
    "# Define categorical and numerical features\n",
    "categorical_features = ['District', 'Make and Model', 'Fuel Type', 'Category']\n",
    "numerical_features = ['Transaction Amount', 'No. of Litres']\n",
    "\n",
    "# Define transformations for categorical and numerical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Split data into features and target variable\n",
    "X = data.drop(['Flag'], axis=1)\n",
    "y = data['Flag']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Define LinearSVC with specific parameters\n",
    "linear_svc = LinearSVC(max_iter=10000, dual=\"auto\", class_weight='balanced', random_state=1)\n",
    "\n",
    "# Create a calibrated classifier with LinearSVC\n",
    "calibrated_svc = CalibratedClassifierCV(estimator=linear_svc, method='sigmoid', cv=5)\n",
    "\n",
    "# Create a pipeline with the calibrated classifier\n",
    "pipeline_svm = Pipeline([\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('classifier', calibrated_svc)\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "pipeline_svm.fit(X_train, y_train)\n",
    "\n",
    "# 4. Model Evaluation\n",
    "# Predictions\n",
    "y_pred = pipeline_svm.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7ea9f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Linear SVM:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.97      0.96     10900\n",
      "        True       0.99      0.98      0.98     28536\n",
      "\n",
      "    accuracy                           0.98     39436\n",
      "   macro avg       0.97      0.98      0.97     39436\n",
      "weighted avg       0.98      0.98      0.98     39436\n",
      "\n",
      "Confusion Matrix:\n",
      " [[10574   326]\n",
      " [  528 28008]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluation\n",
    "print(\"Classification Report for Linear SVM:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23212a5",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9440a5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Naive Bayes:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.97      0.96     10900\n",
      "        True       0.99      0.98      0.98     28536\n",
      "\n",
      "    accuracy                           0.98     39436\n",
      "   macro avg       0.97      0.98      0.97     39436\n",
      "weighted avg       0.98      0.98      0.98     39436\n",
      "\n",
      "Confusion Matrix for Naive Bayes:\n",
      " [[10563   337]\n",
      " [  531 28005]]\n"
     ]
    }
   ],
   "source": [
    "# 1. Import Libraries for Naive Bayes\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "# Define a custom transformer to convert sparse matrix to dense\n",
    "class DenseTransformer(TransformerMixin):\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.toarray()\n",
    "\n",
    "\n",
    "# Preprocessors for numerical and categorical features for Naive Bayes\n",
    "numeric_transformer_nb = Pipeline(steps=[\n",
    "    ('scaler_nb', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer_nb = Pipeline(steps=[\n",
    "    ('onehot_nb', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ('to_dense_nb', DenseTransformer())  # Convert to dense\n",
    "])\n",
    "\n",
    "# Combine preprocessors for Naive Bayes\n",
    "preprocessor_nb = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num_nb', numeric_transformer_nb, numerical_features),\n",
    "        ('cat_nb', categorical_transformer_nb, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Split data into features and target variable for Naive Bayes\n",
    "X_nb = data.drop('Flag', axis=1)\n",
    "y_nb = data['Flag']\n",
    "\n",
    "# Split data into training and testing sets for Naive Bayes\n",
    "X_train_nb, X_test_nb, y_train_nb, y_test_nb = train_test_split(X_nb, y_nb, test_size=0.2, random_state=1)\n",
    "\n",
    "# 3. Model Training for Naive Bayes\n",
    "# Create a pipeline for Naive Bayes - switch out to use CategoricalNB for categorical features (more efficient)\n",
    "pipeline_nb = make_pipeline(\n",
    "    preprocessor_nb,\n",
    "    # GaussianNB()  # Using GaussianNB for numerical features (swap out for CategoricalNB since categorical features)\n",
    ")\n",
    "\n",
    "# Fit the model for Naive Bayes\n",
    "pipeline_nb.fit(X_train_nb, y_train_nb)\n",
    "\n",
    "# 4. Model Evaluation for Naive Bayes\n",
    "# Predictions for Naive Bayes\n",
    "y_pred_nb = pipeline_nb.predict(X_test_nb)\n",
    "\n",
    "# Evaluation metrics for Naive Bayes\n",
    "print(\"Classification Report for Naive Bayes:\\n\", classification_report(y_test_nb, y_pred_nb))\n",
    "print(\"Confusion Matrix for Naive Bayes:\\n\", confusion_matrix(y_test_nb, y_pred_nb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da064a1",
   "metadata": {},
   "source": [
    "# XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21f9d379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for XGBoost:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      0.97      0.96     10900\n",
      "        True       0.99      0.98      0.99     28536\n",
      "\n",
      "    accuracy                           0.98     39436\n",
      "   macro avg       0.97      0.98      0.98     39436\n",
      "weighted avg       0.98      0.98      0.98     39436\n",
      "\n",
      "Confusion Matrix for XGBoost:\n",
      " [[10565   335]\n",
      " [  454 28082]]\n"
     ]
    }
   ],
   "source": [
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define transformations for categorical and numerical features\n",
    "preprocessor_xgb = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num_xgb', StandardScaler(), numerical_features),\n",
    "        ('cat_xgb', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Split data into features and target variable\n",
    "X_xgb = data.drop('Flag', axis=1)\n",
    "y_xgb = data['Flag']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(X_xgb, y_xgb, test_size=0.2, random_state=1)\n",
    "\n",
    "# Calculate the scale_pos_weight value\n",
    "scale_pos_weight = sum(y_train_xgb == 0) / sum(y_test_xgb == 1)\n",
    "\n",
    "# 3. Model Training and Hyperparameter Tuning\n",
    "# Create a pipeline\n",
    "pipeline_xgb = Pipeline([\n",
    "    ('preprocessor_xgb', preprocessor_xgb),\n",
    "    ('classifier_xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss', scale_pos_weight=scale_pos_weight))\n",
    "])\n",
    "\n",
    "# Optional: Define parameters for GridSearchCV\n",
    "param_grid_xgb = {\n",
    "    'classifier_xgb__n_estimators': [50, 100, 150, 200, 300],  # Number of trees\n",
    "    'classifier_xgb__learning_rate': [0.001, 0.01, 0.1, 0.5],  # Learning rate\n",
    "}\n",
    "\n",
    "# Optional: Create GridSearchCV object\n",
    "grid_search_xgb = GridSearchCV(pipeline_xgb, param_grid_xgb, cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit the model (use grid_search_xgb.fit(X_train_xgb, y_train_xgb) if using GridSearchCV)\n",
    "pipeline_xgb.fit(X_train_xgb, y_train_xgb)\n",
    "\n",
    "# 4. Model Evaluation\n",
    "# Predictions (use grid_search_xgb.predict(X_test_xgb) if using GridSearchCV)\n",
    "y_pred_xgb = pipeline_xgb.predict(X_test_xgb)\n",
    "\n",
    "# Evaluation metrics\n",
    "print(\"Classification Report for XGBoost:\\n\", classification_report(y_test_xgb, y_pred_xgb))\n",
    "print(\"Confusion Matrix for XGBoost:\\n\", confusion_matrix(y_test_xgb, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31969894",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6538af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Logistic Regression:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.97      0.96     10900\n",
      "        True       0.99      0.98      0.99     28536\n",
      "\n",
      "    accuracy                           0.98     39436\n",
      "   macro avg       0.97      0.98      0.97     39436\n",
      "weighted avg       0.98      0.98      0.98     39436\n",
      "\n",
      "Confusion Matrix for Logistic Regression:\n",
      " [[10575   325]\n",
      " [  522 28014]]\n"
     ]
    }
   ],
   "source": [
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define transformations for categorical and numerical features\n",
    "preprocessor_lr = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num_lr', StandardScaler(), numerical_features),\n",
    "        ('cat_lr', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Split data into features and target variable\n",
    "X_lr = data.drop('Flag', axis=1)\n",
    "y_lr = data['Flag']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X_lr, y_lr, test_size=0.2, random_state=1)\n",
    "\n",
    "# 3. Model Training\n",
    "# Create a pipeline with an increased max_iter\n",
    "pipeline_lr = Pipeline([\n",
    "    ('preprocessor_lr', preprocessor_lr),\n",
    "    ('classifier_lr', LogisticRegression(max_iter=1000, class_weight='balanced'))  # Increased max_iter\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "pipeline_lr.fit(X_train_lr, y_train_lr)\n",
    "\n",
    "# 4. Model Evaluation\n",
    "# Predictions\n",
    "y_pred_lr = pipeline_lr.predict(X_test_lr)\n",
    "\n",
    "# Evaluation metrics\n",
    "print(\"Classification Report for Logistic Regression:\\n\", classification_report(y_test_lr, y_pred_lr))\n",
    "print(\"Confusion Matrix for Logistic Regression:\\n\", confusion_matrix(y_test_lr, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594bfb3c",
   "metadata": {},
   "source": [
    "# Generating figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "873a58a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af36b4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, model_name, dpi=300):\n",
    "    matrix = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(matrix, annot=True, fmt='g', cmap='viridis')\n",
    "    #plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.xlabel('Predicted Label', size=13)\n",
    "    plt.ylabel('True Label', size=13)\n",
    "    plt.savefig(f'../final_plots/modelling/{model_name}_confusion_matrix.pdf', format='pdf', dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc_curve(y_true, y_scores, model_name, dpi=300):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', size=13)\n",
    "    plt.ylabel('True Positive Rate', size=13)\n",
    "    #plt.title(f'ROC Curve for {model_name}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(f'../final_plots/modelling/{model_name}_roc_curve.pdf', format='pdf', dpi=dpi)\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "839b67f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores_lr = pipeline_lr.predict_proba(X_test_lr)[:, 1]\n",
    "plot_confusion_matrix(y_test, y_pred_lr, \"Logistic_Regression\")\n",
    "plot_roc_curve(y_test, y_scores_lr, \"Logistic_Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "853f58db",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores_nb = pipeline_nb.predict_proba(X_test_nb)[:, 1]\n",
    "plot_confusion_matrix(y_test, y_pred_nb, \"Naive Bayes\")\n",
    "plot_roc_curve(y_test, y_scores_nb, \"Naive Bayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0392976",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores_svm = pipeline_svm.predict_proba(X_test)[:, 1]\n",
    "plot_confusion_matrix(y_test, y_pred, \"Linear_SVM\")\n",
    "plot_roc_curve(y_test, y_scores_svm, \"Linear_SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36028968",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores_xgb = pipeline_xgb.predict_proba(X_test_xgb)[:, 1]\n",
    "plot_confusion_matrix(y_test, y_pred_xgb, \"XGBoost\")\n",
    "plot_roc_curve(y_test, y_scores_xgb, \"XGBoost\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scientificProject)",
   "language": "python",
   "name": "scientificproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
